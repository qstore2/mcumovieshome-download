{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mcumovieshome.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgmDbgM5mKJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Setup\n",
        "!apt-get install xmlstarlet\n",
        "!pip install dask\n",
        "!pip install pandas\n",
        "!pip install beautifulsoup4\n",
        "!rm -rf out feed*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T04mDb9x1SYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Get links\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import dask\n",
        "import dask.dataframe as dd\n",
        "from dask.dataframe.utils import make_meta\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import xml.etree.cElementTree as et\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "STARTPAGE = 1 #@param {type:\"number\"}\n",
        "ENDPAGE = 71 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "WORKERS = 64 #@param {type:\"number\"}\n",
        "dask.config.set(work_stealing=False)\n",
        "dask.config.set(num_workers=WORKERS)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def getmcuredirect(url):\n",
        "  try:\n",
        "    if 'mcupaste.com' in url:\n",
        "      r = requests.get(url)\n",
        "      soup = BeautifulSoup(r.text, 'html5lib')\n",
        "      return soup.select_one('#raw_content').get_text()\n",
        "    else:\n",
        "      return s\n",
        "  except:\n",
        "    return url \n",
        "\n",
        "\n",
        "def getlinks(row):\n",
        "  try:\n",
        "    soup = BeautifulSoup(row['content'], 'html5lib')\n",
        "    links = soup.findAll('a')\n",
        "\n",
        "    urls=[]\n",
        "    for l in links:\n",
        "      url = l['href']\n",
        "      if not '//mcumovieshome.com' in url:\n",
        "        urls.append(getmcuredirect(url))\n",
        "\n",
        "    row['links'] = ' '.join(urls)\n",
        "  except:\n",
        "    pass\n",
        "  pbar.update(1)\n",
        "  return row\n",
        "\n",
        "\n",
        "dfcols=['title','link','content']\n",
        "df_xml = pd.DataFrame(columns=dfcols)\n",
        "\n",
        "for i in range(STARTPAGE, ENDPAGE+1):\n",
        "  r = requests.get(\"https://mcumovieshome.com/feed/?paged=%d\"%(i))\n",
        "  root = et.fromstring(r.content)\n",
        "  rows = root.findall('.//item')\n",
        "  xml_data = [[row.find('title').text, row.find('link').text, row.find('{http://purl.org/rss/1.0/modules/content/}encoded').text] \n",
        "            for row in rows]\n",
        "  df_xml = df_xml.append(pd.DataFrame(xml_data, columns=dfcols))\n",
        "\n",
        "df_xml['links']=''\n",
        "pbar = tqdm(total=len(df_xml), ncols=80)\n",
        "\n",
        "dd_xml = dd.from_pandas(df_xml, npartitions=WORKERS)\n",
        "df_xml = dd_xml.apply(getlinks, axis=1, meta=make_meta(df_xml)).compute()\n",
        "df_xml = df_xml.drop(columns='content')\n",
        "  \n",
        "df_xml.to_csv('mcumovies.csv', encoding='UTF-8', index=False)\n",
        "print('\\n\\n\\nCreate file mcumovies.csv\\n\\n(On the right, click on \"Files\")')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}